{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import trange, tqdm\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=3)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.rnn(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out\n",
    "\n",
    "# Define custom dataset\n",
    "class PacketCaptureDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_through_origin_with_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the slope of the regression line that goes through the origin\n",
    "    for the given data points (x, y) and return the correlation coefficient.\n",
    "    \n",
    "    Parameters:\n",
    "    x (array-like): Array of predictor values.\n",
    "    y (array-like): Array of response values.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: The slope of the regression line through the origin and the correlation coefficient.\n",
    "    \"\"\"\n",
    "    # Reshape x to be a 2D array with one column (required by lstsq)\n",
    "    x_reshaped = np.array(x).reshape(-1, 1)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Use lstsq to fit a line through the origin\n",
    "    m, _, _, _ = np.linalg.lstsq(x_reshaped, y, rcond=None)\n",
    "    \n",
    "    # m contains the slope of the line through the origin\n",
    "    slope = m[0]\n",
    "    \n",
    "    # Calculate the predicted y values using the slope\n",
    "    y_pred = slope * x\n",
    "    \n",
    "    # Calculate the correlation coefficient\n",
    "    correlation_matrix = np.corrcoef(y, y_pred)\n",
    "    correlation_coefficient = correlation_matrix[0, 1]\n",
    "    \n",
    "    return correlation_coefficient, slope\n",
    "\n",
    "\n",
    "def normalized_slope(slope):\n",
    "    return math.exp(-abs(math.log10(slope)))\n",
    "\n",
    "def tiger_score(correlation_coef, slope):\n",
    "    return normalized_slope(slope) * correlation_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_through_origin_with_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the slope of the regression line that goes through the origin\n",
    "    for the given data points (x, y) and return the correlation coefficient.\n",
    "    \n",
    "    Parameters:\n",
    "    x (array-like): Array of predictor values.\n",
    "    y (array-like): Array of response values.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: The slope of the regression line through the origin and the correlation coefficient.\n",
    "    \"\"\"\n",
    "    # Reshape x to be a 2D array with one column (required by lstsq)\n",
    "    x_reshaped = np.array(x).reshape(-1, 1)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Use lstsq to fit a line through the origin\n",
    "    m, _, _, _ = np.linalg.lstsq(x_reshaped, y, rcond=None)\n",
    "    \n",
    "    # m contains the slope of the line through the origin\n",
    "    slope = m[0]\n",
    "    \n",
    "    # Calculate the predicted y values using the slope\n",
    "    y_pred = slope * x\n",
    "    \n",
    "    # Calculate the correlation coefficient\n",
    "    correlation_matrix = np.corrcoef(y, y_pred)\n",
    "    correlation_coefficient = correlation_matrix[0, 1]\n",
    "    \n",
    "    return correlation_coefficient, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_through_origin_with_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the slope of the regression line that goes through the origin\n",
    "    for the given data points (x, y) and return the correlation coefficient.\n",
    "    \n",
    "    Parameters:\n",
    "    x (array-like): Array of predictor values.\n",
    "    y (array-like): Array of response values.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: The slope of the regression line through the origin and the correlation coefficient.\n",
    "    \"\"\"\n",
    "    # Reshape x to be a 2D array with one column (required by lstsq)\n",
    "    x_reshaped = np.array(x).reshape(-1, 1)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Use lstsq to fit a line through the origin\n",
    "    m, _, _, _ = np.linalg.lstsq(x_reshaped, y, rcond=None)\n",
    "    \n",
    "    # m contains the slope of the line through the origin\n",
    "    slope = m[0]\n",
    "    \n",
    "    # Calculate the predicted y values using the slope\n",
    "    y_pred = slope * x\n",
    "    \n",
    "    # Calculate the correlation coefficient\n",
    "    correlation_matrix = np.corrcoef(y, y_pred)\n",
    "    correlation_coefficient = correlation_matrix[0, 1]\n",
    "    \n",
    "    return correlation_coefficient, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming PacketCaptureDataset, RNNModel, tiger_score, and regression_through_origin_with_correlation are defined elsewhere\n",
    "def load_and_normalize_data(model_name):\n",
    "    data_tensors = torch.load(f'../model/data_tensors_7.5s_0.01s_large.pt')\n",
    "    target_tensors = torch.load(f'../model/target_tensors_7.5s_0.01s_large.pt')\n",
    "    \n",
    "    if isinstance(data_tensors, list):\n",
    "        data_tensors = torch.stack([torch.tensor(t) for t in data_tensors]).to(device)\n",
    "    else:\n",
    "        data_tensors = data_tensors.to(device)\n",
    "    \n",
    "    if isinstance(target_tensors, list):\n",
    "        target_tensors = torch.stack([torch.tensor(t) for t in target_tensors]).to(device)\n",
    "    else:\n",
    "        target_tensors = target_tensors.to(device)\n",
    "    \n",
    "    target_mean = target_tensors.mean()\n",
    "    target_std = target_tensors.std()\n",
    "    target_tensors = (target_tensors - target_mean) / target_std\n",
    "    \n",
    "    return data_tensors, target_tensors, target_mean, target_std\n",
    "\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    return tensor * std + mean\n",
    "\n",
    "def prepare_datasets(data_tensors, target_tensors):\n",
    "    train_data, test_data, train_targets, test_targets = train_test_split(\n",
    "        data_tensors, target_tensors, test_size=0.2, random_state=42\n",
    "    )\n",
    "    return PacketCaptureDataset(train_data, train_targets), PacketCaptureDataset(test_data, test_targets)\n",
    "\n",
    "def evaluate_model(model, test_dataset, mean, std):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_data_tensor = torch.stack([item[0] for item in test_dataset]).to(device)\n",
    "        test_targets_tensor = torch.stack([item[1] for item in test_dataset]).to(device)\n",
    "        pred = model(test_data_tensor).flatten()\n",
    "        pred_denorm = denormalize(pred, mean, std).cpu().numpy().flatten()\n",
    "        targets_denorm = denormalize(test_targets_tensor, mean, std).cpu().numpy().flatten()\n",
    "        return pred_denorm, targets_denorm\n",
    "\n",
    "\n",
    "def calculate_metrics(pred_denorm, targets_denorm):\n",
    "    abs_errors = np.abs(pred_denorm - targets_denorm)\n",
    "    percent_abs_errors = (abs_errors / targets_denorm) * 100\n",
    "    mse = np.mean((pred_denorm - targets_denorm) ** 2)\n",
    "\n",
    "    metrics = {\n",
    "        'avg_abs_error': np.mean(abs_errors),\n",
    "        'avg_abs_error_normalized': np.mean(percent_abs_errors),\n",
    "        'median_abs_error': np.median(abs_errors),\n",
    "        'median_abs_error_normalized': np.median(percent_abs_errors),\n",
    "        '98th_abs_error': np.percentile(abs_errors, 98),\n",
    "        '98th_avg_abs_error': np.mean(abs_errors[abs_errors <= np.percentile(abs_errors, 98)]),\n",
    "        '98th_abs_error_normalized': np.percentile(percent_abs_errors, 98),\n",
    "        '98th_avg_abs_error_normalized': np.mean(percent_abs_errors[percent_abs_errors <= np.percentile(percent_abs_errors, 98)]),\n",
    "        'mse': mse\n",
    "    }\n",
    "    return metrics, abs_errors\n",
    "\n",
    "def calculate_regression_metrics(pred, targets, abs_errors, percentile_98th):\n",
    "    slope, correlation_coef = regression_through_origin_with_correlation(pred, targets)\n",
    "    pred_98th = pred[abs_errors <= percentile_98th]\n",
    "    targets_98th = targets[abs_errors <= percentile_98th]\n",
    "    slope_98th, correlation_coef_98th = regression_through_origin_with_correlation(pred_98th, targets_98th)\n",
    "    return slope, correlation_coef, slope_98th, correlation_coef_98th\n",
    "\n",
    "def append_results(results_df, model_name, total, delta, slope, correlation_coef, tiger_score_value, metrics, slope_98th, correlation_coef_98th, tiger_score_value_98th):\n",
    "    results_df = results_df.append({\n",
    "        'model_name': model_name,\n",
    "        'total': total,\n",
    "        'delta': delta,\n",
    "        'slope': slope,\n",
    "        'correlation_coef': correlation_coef,\n",
    "        'tiger_score': tiger_score_value,\n",
    "        'avg_abs_error': metrics['avg_abs_error'],\n",
    "        'avg_abs_error_normalized': metrics['avg_abs_error_normalized'],\n",
    "        'median_abs_error': metrics['median_abs_error'],\n",
    "        'median_abs_error_normalized': metrics['median_abs_error_normalized'],\n",
    "        '98th_abs_error': metrics['98th_abs_error'],\n",
    "        '98th_avg_abs_error': metrics['98th_avg_abs_error'],\n",
    "        '98th_abs_error_normalized': metrics['98th_abs_error_normalized'],\n",
    "        '98th_avg_abs_error_normalized': metrics['98th_avg_abs_error_normalized'],\n",
    "        'slope_98th': slope_98th,\n",
    "        'correlation_coef_98th': correlation_coef_98th,\n",
    "        'tiger_score_98th': tiger_score_value_98th,\n",
    "        'mse': metrics['mse']\n",
    "    }, ignore_index=True)\n",
    "    return results_df\n",
    "\n",
    "def plot_actual_vs_predicted(targets_denorm, pred_denorm, model_name):\n",
    "    plt.figure()\n",
    "    plt.scatter(targets_denorm, pred_denorm, label='Predicted vs Actual')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'Actual vs Predicted for {model_name}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large_model_epoch_500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36338/2527998908.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_tensors = torch.stack([torch.tensor(t) for t in data_tensors]).to(device)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.15 GiB (GPU 0; 7.78 GiB total capacity; 3.85 GiB already allocated; 1.74 GiB free; 5.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m RNNModel(input_size, hidden_size, output_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../model/large_models/large_models_256/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 33\u001b[0m pred_denorm, targets_denorm \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m correlation_coef, slope \u001b[38;5;241m=\u001b[39m regression_through_origin_with_correlation(pred_denorm, targets_denorm)\n\u001b[1;32m     36\u001b[0m tiger_score_value \u001b[38;5;241m=\u001b[39m tiger_score(correlation_coef, slope)\n",
      "Cell \u001b[0;32mIn[32], line 38\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_dataset, mean, std)\u001b[0m\n\u001b[1;32m     36\u001b[0m test_data_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m test_dataset])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m test_targets_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([item[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m test_dataset])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 38\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     39\u001b[0m pred_denorm \u001b[38;5;241m=\u001b[39m denormalize(pred, mean, std)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     40\u001b[0m targets_denorm \u001b[38;5;241m=\u001b[39m denormalize(test_targets_tensor, mean, std)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[28], line 9\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     _, (h_n, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(h_n[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.15 GiB (GPU 0; 7.78 GiB total capacity; 3.85 GiB already allocated; 1.74 GiB free; 5.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'model_name', 'total', 'delta', 'slope', 'correlation_coef', 'tiger_score', \n",
    "    'avg_abs_error', 'avg_abs_error_normalized', 'median_abs_error', \n",
    "    'median_abs_error_normalized', '98th_abs_error', '98th_avg_abs_error',\n",
    "    '98th_abs_error_normalized', '98th_avg_abs_error_normalized',\n",
    "    'slope_98th', 'correlation_coef_98th', 'tiger_score_98th', 'mse'\n",
    "])\n",
    "\n",
    "hidden_size = 256\n",
    "\n",
    "for filename in os.listdir('../model/large_models/large_models_256'):\n",
    "    if not filename.startswith('large_epoch_') or not filename.endswith('.pt'):\n",
    "        continue\n",
    "\n",
    "    epoch = int(filename.split('_')[-1].split('.')[0])\n",
    "    model_name = f'large_model_epoch_{epoch}'\n",
    "    print(model_name)\n",
    "    \n",
    "    # Assuming 'total' and 'delta' can be derived or are fixed values for this set of models\n",
    "    total = 100  # Replace with actual value or logic to derive it\n",
    "    delta = 0.01  # Replace with actual value or logic to derive it\n",
    "    data_tensors, target_tensors, target_mean, target_std = load_and_normalize_data(model_name)\n",
    "    train_dataset, test_dataset = prepare_datasets(data_tensors, target_tensors)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    input_size = len(train_dataset[0][0][0])\n",
    "    output_size = 1\n",
    "    model = RNNModel(input_size, hidden_size, output_size).to(device)\n",
    "    model.load_state_dict(torch.load(f'../model/large_models/large_models_256/{filename}'))\n",
    "    \n",
    "    pred_denorm, targets_denorm = evaluate_model(model, test_dataset, target_mean, target_std)\n",
    "    \n",
    "    correlation_coef, slope = regression_through_origin_with_correlation(pred_denorm, targets_denorm)\n",
    "    tiger_score_value = tiger_score(correlation_coef, slope)\n",
    "    \n",
    "    metrics, abs_errors = calculate_metrics(pred_denorm, targets_denorm)\n",
    "    slope_98th, correlation_coef_98th = calculate_regression_metrics(pred_denorm, targets_denorm, abs_errors, metrics['98th_abs_error'])\n",
    "    tiger_score_value_98th = tiger_score(correlation_coef_98th, slope_98th)\n",
    "    \n",
    "    results_df = append_results(results_df, model_name, total, delta, slope, correlation_coef, tiger_score_value, metrics, slope_98th, correlation_coef_98th, tiger_score_value_98th)\n",
    "    \n",
    "    plot_actual_vs_predicted(targets_denorm, pred_denorm, model_name)\n",
    "    \n",
    "    print(f'Slope: {slope}, Correlation Coefficient: {correlation_coef}, Tiger Score: {tiger_score_value}, MSE: {metrics[\"mse\"]}')\n",
    "    print(f'Tiger Score 98th percentile for {model_name}: {tiger_score_value_98th}')\n",
    "\n",
    "results_df.to_csv('large_model_evaluation_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

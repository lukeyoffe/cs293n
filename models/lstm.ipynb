{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from csv_to_vec import calculate_features\n",
    "import os\n",
    "from tqdm import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.rnn(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define custom dataset\n",
    "class PacketCaptureDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.35it/s]\n"
     ]
    }
   ],
   "source": [
    "packet_capture_data = []  # List of lists of dicts\n",
    "for i in trange(100):\n",
    "    filename = f\"../dfs/out{i}.csv\"\n",
    "    df = pd.read_csv(filename)\n",
    "    features = calculate_features(df, 1)\n",
    "    packet_capture_data.append(features)\n",
    "\n",
    "\n",
    "speed_test_results = json.load(open(\"../dfs/ground_truths.json\"))[:100]\n",
    "\n",
    "\n",
    "# Convert data to tensors\n",
    "data_tensors = []\n",
    "for packet_capture in packet_capture_data:\n",
    "    packet_capture_tensor = torch.tensor(\n",
    "        [list(timeslice.values()) for timeslice in packet_capture], dtype=torch.float32\n",
    "    )\n",
    "    data_tensors.append(packet_capture_tensor)\n",
    "target_tensors = torch.tensor(speed_test_results, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_targets, test_targets = train_test_split(\n",
    "    data_tensors, target_tensors, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "train_data = [data.to(device) for data in train_data]\n",
    "train_targets = train_targets.to(device)\n",
    "test_data = [data.to(device) for data in test_data]\n",
    "test_targets = test_targets.to(device)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = PacketCaptureDataset(train_data, train_targets)\n",
    "test_dataset = PacketCaptureDataset(test_data, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(\n",
    "    packet_capture_data[0][0]\n",
    ")  # Assuming all packet captures have the same structure\n",
    "hidden_size = 768\n",
    "output_size = 1\n",
    "num_layers = 4\n",
    "model = RNNModel(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 6100.870697021484\n",
      "Epoch 20, Loss: 9225.327545166016\n",
      "Epoch 30, Loss: 7803.260772705078\n",
      "Epoch 40, Loss: 6037.5985107421875\n",
      "Epoch 50, Loss: 5992.712020874023\n",
      "Epoch 60, Loss: 5988.865737915039\n",
      "Epoch 70, Loss: 6006.6195068359375\n",
      "Epoch 80, Loss: 6135.953643798828\n",
      "Epoch 90, Loss: 7933.091796875\n",
      "Epoch 100, Loss: 7805.915222167969\n",
      "Epoch 110, Loss: 5856.771942138672\n",
      "Epoch 120, Loss: 7619.424697875977\n",
      "Epoch 130, Loss: 7398.637298583984\n",
      "Epoch 140, Loss: 5342.2714920043945\n",
      "Epoch 150, Loss: 5108.782562255859\n",
      "Epoch 160, Loss: 5335.70751953125\n",
      "Epoch 170, Loss: 4927.200592041016\n",
      "Epoch 180, Loss: 4797.329238891602\n",
      "Epoch 190, Loss: 4572.948623657227\n",
      "Epoch 200, Loss: 6628.154102325439\n",
      "Epoch 210, Loss: 4256.708694458008\n",
      "Epoch 220, Loss: 4150.814239501953\n",
      "Epoch 230, Loss: 3948.1390419006348\n",
      "Epoch 240, Loss: 7422.523239135742\n",
      "Epoch 250, Loss: 3671.971363067627\n",
      "Epoch 260, Loss: 5572.993408203125\n",
      "Epoch 270, Loss: 3356.458786010742\n",
      "Epoch 280, Loss: 3082.106185913086\n",
      "Epoch 290, Loss: 3935.280153274536\n",
      "Epoch 300, Loss: 2251.2354850769043\n",
      "Epoch 310, Loss: 2012.0518970489502\n",
      "Epoch 320, Loss: 1822.7647743225098\n",
      "Epoch 330, Loss: 2563.0890169143677\n",
      "Epoch 340, Loss: 2898.21613407135\n",
      "Epoch 350, Loss: 2060.5515599250793\n",
      "Epoch 360, Loss: 1175.6834993362427\n",
      "Epoch 370, Loss: 1068.0679054260254\n",
      "Epoch 380, Loss: 1562.3247337341309\n",
      "Epoch 390, Loss: 1378.2972778975964\n",
      "Epoch 400, Loss: 748.1248016059399\n",
      "Epoch 410, Loss: 1097.3491002321243\n",
      "Epoch 420, Loss: 590.423403263092\n",
      "Epoch 430, Loss: 528.9004139900208\n",
      "Epoch 440, Loss: 783.3926360607147\n",
      "Epoch 450, Loss: 407.7288013100624\n",
      "Epoch 460, Loss: 349.91169430315495\n",
      "Epoch 470, Loss: 311.23573100566864\n",
      "Epoch 480, Loss: 344.9677700996399\n",
      "Epoch 490, Loss: 242.06097161769867\n",
      "Epoch 500, Loss: 384.39322862029076\n",
      "Epoch 510, Loss: 327.9265012741089\n",
      "Epoch 520, Loss: 156.48165494203568\n",
      "Epoch 530, Loss: 154.45708402991295\n",
      "Epoch 540, Loss: 114.81673551537097\n",
      "Epoch 550, Loss: 102.00915145874023\n",
      "Epoch 560, Loss: 158.98470458388329\n",
      "Epoch 570, Loss: 67.76087635755539\n",
      "Epoch 580, Loss: 58.515814423561096\n",
      "Epoch 590, Loss: 48.57985021173954\n",
      "Epoch 600, Loss: 39.23441535234451\n",
      "Epoch 610, Loss: 31.693260312080383\n",
      "Epoch 620, Loss: 26.45355975627899\n",
      "Epoch 630, Loss: 23.0667472332716\n",
      "Epoch 640, Loss: 20.70309019088745\n",
      "Epoch 650, Loss: 16.851380951702595\n",
      "Epoch 660, Loss: 13.79762214049697\n",
      "Epoch 670, Loss: 9.458595424890518\n",
      "Epoch 680, Loss: 7.913618847727776\n",
      "Epoch 690, Loss: 5.919474810361862\n",
      "Epoch 700, Loss: 4.6990978214889765\n",
      "Epoch 710, Loss: 4.345037028193474\n",
      "Epoch 720, Loss: 5.327840089797974\n",
      "Epoch 730, Loss: 5.198145925998688\n",
      "Epoch 740, Loss: 4.510693073272705\n",
      "Epoch 750, Loss: 4.359528407454491\n",
      "Epoch 760, Loss: 1.9208251237869263\n",
      "Epoch 770, Loss: 2.569388519972563\n",
      "Epoch 780, Loss: 1.1537689939141273\n",
      "Epoch 790, Loss: 1.7337078154087067\n",
      "Epoch 800, Loss: 0.8504945114254951\n",
      "Epoch 810, Loss: 0.6086716987192631\n",
      "Epoch 820, Loss: 1.2906266748905182\n",
      "Epoch 830, Loss: 1.6357179284095764\n",
      "Epoch 840, Loss: 0.7782922461628914\n",
      "Epoch 850, Loss: 0.8602740913629532\n",
      "Epoch 860, Loss: 0.33359479159116745\n",
      "Epoch 870, Loss: 0.24402474611997604\n",
      "Epoch 880, Loss: 0.14827485475689173\n",
      "Epoch 890, Loss: 0.33688145130872726\n",
      "Epoch 900, Loss: 0.31899547576904297\n",
      "Epoch 910, Loss: 0.2814418002963066\n",
      "Epoch 920, Loss: 0.22173626720905304\n",
      "Epoch 930, Loss: 1.352422572672367\n",
      "Epoch 940, Loss: 0.13018662482500076\n",
      "Epoch 950, Loss: 0.18624820187687874\n",
      "Epoch 960, Loss: 0.14302178472280502\n",
      "Epoch 970, Loss: 0.3582214154303074\n",
      "Epoch 980, Loss: 0.14320727065205574\n",
      "Epoch 990, Loss: 0.5371834896504879\n",
      "Epoch 1000, Loss: 0.31356875225901604\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss per Example: 600.16220703125\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "num_examples = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        # print(inputs)\n",
    "        # print(inputs.size())\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        test_loss += loss.item()\n",
    "        num_examples += inputs.size(0)\n",
    "\n",
    "average_test_loss = test_loss / num_examples\n",
    "print(f\"Average Test Loss per Example: {average_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
